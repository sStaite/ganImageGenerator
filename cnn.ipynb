{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seamus/opt/anaconda3/envs/ptenv/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# cnn in pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_trainset = pd.read_csv(\"data/mnist_train.csv\")\n",
    "mnist_testset = pd.read_csv(\"data/mnist_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = mnist_trainset[\"label\"].to_numpy()\n",
    "test_labels = mnist_testset[\"label\"].to_numpy()\n",
    "\n",
    "train_images = mnist_trainset.drop(columns=[\"label\"]).to_numpy().reshape(60000, 28, 28) / 255.0\n",
    "test_images = mnist_testset.drop(columns=[\"label\"]).to_numpy().reshape(10000, 28, 28) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f95f8cc7610>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMkAAADICAYAAABCmsWgAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAOXUlEQVR4nO3df0jU9x8H8Ofl8qaiFxLeecuaG5asmJGoIJaO4Q1hQrb9Y/+0H2y1NBKhyPlHN2gqtomEto0R2gau/nHl9sfmgXZuyGI5W6EgDKzc8pA2vTMzRX1//xjed9fnbW9PP3qf0+cDPn/4uvddr3f49O3n4+eHSQghQEQL2hDqBoiMjiEhUmBIiBQYEiIFhoRIgSEhUmBIiBQYEiIFhoRIgSEhUnhmpT74/PnzOHv2LIaHh7Fz507U19dj7969yvfNzc3h/v37iI2NhclkWqn2aJ0TQmB8fBx2ux0bNijWCrECLl26JDZu3Ci+/PJL0d/fL44fPy5iYmLE3bt3le8dGhoSALhxW5VtaGhI+T25IiHJzMwUR44cCailpqaKU6dOKd87NjYW8v84butnGxsbU35P6r5PMj09jZ6eHjgcjoC6w+FAd3e3ZvzU1BR8Pp9/Gx8f17slogUt5ld63UPy4MEDzM7Owmq1BtStVis8Ho9mfHV1NSwWi39LSkrSuyWiZVmxo1tPJlQIIU1tRUUFvF6vfxsaGlqploiWRPejW5s3b0ZERIRm1RgZGdGsLgBgNpthNpv1boNIN7qvJJGRkUhPT4fL5Qqou1wuZGdn6/3PEa28pR7Bepr5Q8AXLlwQ/f39oqysTMTExIg7d+4o3+v1ekN+xIPb+tm8Xq/ye3JFQiKEEI2NjWLbtm0iMjJS7NmzR7jd7kW9jyHhtprbYkJiEsJYN4Lw+XywWCyhboPWCa/Xi7i4uKeO4blbRAoMCZECQ0KkwJAQKTAkRAoMCZECQ0KkwJAQKTAkRAoMCZECQ0KkwJAQKTAkRAoMCZECQ0KkwJAQKTAkRAordi9gWr6IiAhNTY+rNktLSzW16Oho6dgdO3ZoaiUlJdKxn3zyiaZWXFwsHfv48WNNraamRjr2o48+ktZXC1cSIgWGhEiBISFSYEiIFBgSIgUe3dLB1q1bNbXIyEjpWNmtXnNycqRjN23apKm98cYbwTW3TH/++aemdu7cOenYoqIiTW2hR2n8/vvvmprb7Q6yu9XBlYRIgSEhUmBIiBQYEiIF3jA7CLt375bWOzo6NDWjzmEhc3Nz0vo777yjqT18+HDRnzs8PCytj46OamoDAwOL/ly98IbZRDpgSIgUGBIiBYaESIEhIVLgaSlBuHfvnrT+999/a2qrfXTr+vXrmtrY2Jh07CuvvKKpTU9PS8d+/fXXy+prLeBKQqTAkBApMCRECgwJkQJ33IPwzz//SOsnTpzQ1F5//XXp2N7eXk1toeszZG7evCmt5+fna2oTExPSsTt37tTUjh8/vuge1huuJEQKDAmRAkNCpMCQECkEHZKuri4UFhbCbrfDZDLhypUrAa8LIeB0OmG32xEVFYW8vDz09fXp1S/Rqgv66NbExATS0tLw9ttvS+/cUVtbi7q6OjQ3N2P79u04c+YM8vPzMTAwgNjYWF2aNponf1AA8guxAPndQ9LS0qRj3333XU1Ndr9dYOEjWTKyH1rvv//+ot+/3gQdkoKCAhQUFEhfE0Kgvr4elZWVOHDgAADg4sWLsFqtaGlpweHDh5fXLVEI6LpPMjg4CI/HA4fD4a+ZzWbk5uaiu7tb+p6pqSn4fL6AjchIdA2Jx+MBAFit1oC61Wr1v/ak6upqWCwW/5aUlKRnS0TLtiJHt0wmU8DXQghNbV5FRQW8Xq9/GxoaWomWiJZM19NSbDYbgH9XlMTERH99ZGREs7rMM5vNMJvNerZhCMH82uj1ehc99r333pPWL1++rKktdAcUCo6uK0lycjJsNhtcLpe/Nj09DbfbLb0HLlE4CHolefjwIf744w//14ODg7h58ybi4+OxdetWlJWVoaqqCikpKUhJSUFVVRWio6Nx8OBBXRsnWi1Bh+TGjRsBl3+Wl5cDAA4dOoTm5macPHkSk5OTOHr0KEZHR5GVlYX29vY1+zcSWvuCDkleXh6edtNHk8kEp9MJp9O5nL6IDIPnbhEp8F7ABhATEyOtf/fdd5pabm6udKzsLIj29vblNbYO8F7ARDpgSIgUGBIiBYaESIE77gb24osvamq//fabdKzslqadnZ3SsTdu3NDUGhsbpWMN9u2hO+64E+mAISFSYEiIFBgSIgWGhEiBR7fCTFFRkbTe1NSkqQVz5vWHH34orX/11Vea2kKPnQ5HPLpFpAOGhEiBISFSYEiIFLjjvkbs2rVLU6urq5OOffXVVxf9uV988YWm9vHHH0vH/vXXX4v+XKPgjjuRDhgSIgWGhEiBISFSYEiIFHh0aw3btGmTtF5YWKipyU5rAbQ3PwcWfkCR7DHZRsejW0Q6YEiIFBgSIgWGhEiBO+4E4N9nV8o884z2nuozMzPSsa+99pqmdu3atWX1tdK4406kA4aESIEhIVJgSIgUGBIiBV0fUU2h8/LLL2tqb775pnRsRkaGpiY7irWQ/v5+ab2rq2vRnxFOuJIQKTAkRAoMCZECQ0KkwB13A9uxY4emVlpaKh174MABTc1msy27h9nZWU1toduczs3NLfvfMyKuJEQKDAmRAkNCpMCQECkEFZLq6mpkZGQgNjYWCQkJ2L9/PwYGBgLGCCHgdDpht9sRFRWFvLw89PX16do00WoK6uiW2+1GSUkJMjIyMDMzg8rKSjgcDvT39yMmJgYAUFtbi7q6OjQ3N2P79u04c+YM8vPzMTAwENRDZdYq2RGn4uJi6VjZkaznn39e75YAyB9bDcjv+9vW1rYiPRhVUCH54YcfAr5uampCQkICenp6sG/fPgghUF9fj8rKSv8hyYsXL8JqtaKlpQWHDx/Wr3OiVbKsfRKv1wsAiI+PBwAMDg7C4/HA4XD4x5jNZuTm5qK7u1v6GVNTU/D5fAEbkZEsOSRCCJSXlyMnJ8d/23+PxwMAsFqtAWOtVqv/tSdVV1fDYrH4t6SkpKW2RLQilhyS0tJS3Lp1C998843mtSfv+ieEkN4JEAAqKirg9Xr929DQ0FJbIloRSzot5dixY2hra0NXVxe2bNnir8/vlHo8HiQmJvrrIyMjmtVlntlshtlsXkobhiGb20svvSQd29DQoKmlpqbq3hMAXL9+XVo/e/aspnb16lXp2LV6qkkwglpJhBAoLS1Fa2srOjo6kJycHPB6cnIybDYbXC6XvzY9PQ23243s7Gx9OiZaZUGtJCUlJWhpacHVq1cRGxvr38+wWCyIioqCyWRCWVkZqqqqkJKSgpSUFFRVVSE6OhoHDx5ckQkQrbSgQvLZZ58BAPLy8gLqTU1NeOuttwAAJ0+exOTkJI4ePYrR0VFkZWWhvb2dfyOhsBVUSBZzs0eTyQSn0wmn07nUnogMheduESnwoqsFzP+B9L9kj2sGgN27d2tqL7zwgt4tAcCCf5T99NNPNbUff/xROnZyclLXntY6riRECgwJkQJDQqTAkBAprKsd96ysLE3txIkT0rGZmZma2nPPPad7TwDw6NEjaf3cuXOaWlVVlXTsxMSErj3R/3ElIVJgSIgUGBIiBYaESIEhIVJYV0e3ioqKFlULluyhNt9//710rOzxzrJTSgBgbGxsWX2RPriSECkwJEQKDAmRAkNCpGASi7nccBX5fD5YLJZQt0HrhNfrRVxc3FPHcCUhUmBIiBQYEiIFhoRIgSEhUmBIiBQYEiIFhoRIgSEhUmBIiBQYEiIFhoRIgSEhUmBIiBQYEiIFw4XEYJe30Bq3mO83w4VkfHw81C3QOrKY7zfDXZk4NzeH+/fvIzY2FuPj40hKSsLQ0JDy6rFw4/P5OLcQEkJgfHwcdrsdGzY8fa0w3H23NmzYgC1btgD49yGlABAXF2fY/+zl4txCZ7GXiRvu1y0io2FIiBQMHRKz2YzTp0/DbDaHuhXdcW7hw3A77kRGY+iVhMgIGBIiBYaESIEhIVIwdEjOnz+P5ORkPPvss0hPT8dPP/0U6paC1tXVhcLCQtjtdphMJly5ciXgdSEEnE4n7HY7oqKikJeXh76+vtA0G4Tq6mpkZGQgNjYWCQkJ2L9/PwYGBgLGhOvcnmTYkFy+fBllZWWorKxEb28v9u7di4KCAty7dy/UrQVlYmICaWlpaGhokL5eW1uLuro6NDQ04Ndff4XNZkN+fr7hz2Fzu90oKSnBL7/8ApfLhZmZGTgcjoDnyYfr3DSEQWVmZoojR44E1FJTU8WpU6dC1NHyARDffvut/+u5uTlhs9lETU2Nv/b48WNhsVjE559/HoIOl25kZEQAEG63WwixtuZmyJVkenoaPT09cDgcAXWHw4Hu7u4QdaW/wcFBeDyegHmazWbk5uaG3Ty9Xi8AID4+HsDampshQ/LgwQPMzs7CarUG1K1WKzweT4i60t/8XMJ9nkIIlJeXIycnB7t27QKwduYGGPAs4P+aPwt4nhBCU1sLwn2epaWluHXrFn7++WfNa+E+N8CgK8nmzZsRERGh+YkzMjKi+ckUzmw2GwCE9TyPHTuGtrY2dHZ2+i9xANbG3OYZMiSRkZFIT0+Hy+UKqLtcLmRnZ4eoK/0lJyfDZrMFzHN6ehput9vw8xRCoLS0FK2trejo6EBycnLA6+E8N42QHjZ4ikuXLomNGzeKCxcuiP7+flFWViZiYmLEnTt3Qt1aUMbHx0Vvb6/o7e0VAERdXZ3o7e0Vd+/eFUIIUVNTIywWi2htbRW3b98WxcXFIjExUfh8vhB3/nQffPCBsFgs4tq1a2J4eNi/PXr0yD8mXOf2JMOGRAghGhsbxbZt20RkZKTYs2eP//BiOOns7BQANNuhQ4eEEP8eKj19+rSw2WzCbDaLffv2idu3b4e26UWQzQmAaGpq8o8J17k9iafKEykYcp+EyEgYEiIFhoRIgSEhUmBIiBQYEiIFhoRIgSEhUmBIiBQYEiIFhoRIgSEhUvgf0fv4xupXHrEAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(2,2))\n",
    "plt.imshow(train_images[0], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(((54000, 28, 28), (54000,)), ((6000, 28, 28), (6000,)))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im_train, im_val, lab_train, lab_val = train_test_split(train_images, train_labels, test_size = 0.1)\n",
    "(im_train.shape, lab_train.shape), (im_val.shape, lab_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to torch format\n",
    "im_train = im_train.reshape(54000, 1, 28, 28).astype(np.float32)\n",
    "im_train = torch.from_numpy(im_train)\n",
    "\n",
    "lab_train = torch.from_numpy(lab_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_val = im_val.reshape(6000, 1, 28, 28).astype(np.float32)\n",
    "im_val = torch.from_numpy(im_val)\n",
    "\n",
    "lab_val = torch.from_numpy(lab_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.cnn_layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=4, kernel_size=3, padding=1), #one input channel (grayscale), 32 filters, 3x3 kernels\n",
    "            nn.BatchNorm2d(4), \n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(in_channels=4, out_channels=8, kernel_size=3, padding=1), #one input channel (grayscale), 32 filters, 3x3 kernels\n",
    "            nn.BatchNorm2d(8), \n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "        self.linear_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=8 * 7 * 7, out_features=64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_features=64, out_features=10)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.cnn_layers(x)\n",
    "        x = self.linear_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.002)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"hi\")\n",
    "    model = model.cuda()\n",
    "    loss = loss.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, train_losses, val_losses):\n",
    "    model.train()\n",
    "    tr_loss = 0\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        im_train.cuda()\n",
    "        lab_train.cuda()\n",
    "        im_val.cuda()\n",
    "        lab_val.cuda()\n",
    "    \n",
    "    optimizer.zero_grad() # clearing the gradients of the model parameters\n",
    "\n",
    "    # predictions\n",
    "    output_train = model(im_train)\n",
    "    output_val = model(im_val)\n",
    "    \n",
    "    # compute loss\n",
    "    loss_train = loss(output_train, lab_train)\n",
    "    loss_val = loss(output_val, lab_val)\n",
    "    train_losses.append(loss_train)\n",
    "    val_losses.append(loss_val)\n",
    "\n",
    "    # update all parameter weights\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "    tr_loss = loss_train.item()\n",
    "\n",
    "    print(f\"Epoch: {epoch+1}, Loss: {loss_val}\")\n",
    "    return loss_val\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    n_epochs = 100\n",
    "    prev_loss = 100 #number larger than any loss possible\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        loss = train(epoch, train_losses, val_losses)\n",
    "        if loss > prev_loss:\n",
    "            break\n",
    "        prev_loss = loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 2.3297336101531982\n",
      "Epoch: 2, Loss: 2.182982921600342\n",
      "Epoch: 3, Loss: 2.0637834072113037\n",
      "Epoch: 4, Loss: 1.9351667165756226\n",
      "Epoch: 5, Loss: 1.7973912954330444\n",
      "Epoch: 6, Loss: 1.6606872081756592\n",
      "Epoch: 7, Loss: 1.5224246978759766\n",
      "Epoch: 8, Loss: 1.384137511253357\n",
      "Epoch: 9, Loss: 1.254077672958374\n",
      "Epoch: 10, Loss: 1.1351348161697388\n",
      "Epoch: 11, Loss: 1.0274492502212524\n",
      "Epoch: 12, Loss: 0.9320433139801025\n",
      "Epoch: 13, Loss: 0.8479202389717102\n",
      "Epoch: 14, Loss: 0.7721070051193237\n",
      "Epoch: 15, Loss: 0.7049536108970642\n",
      "Epoch: 16, Loss: 0.6462365984916687\n",
      "Epoch: 17, Loss: 0.5954270362854004\n",
      "Epoch: 18, Loss: 0.5517388582229614\n",
      "Epoch: 19, Loss: 0.5136697888374329\n",
      "Epoch: 20, Loss: 0.4803393483161926\n",
      "Epoch: 21, Loss: 0.4511268436908722\n",
      "Epoch: 22, Loss: 0.4249560534954071\n",
      "Epoch: 23, Loss: 0.40167146921157837\n",
      "Epoch: 24, Loss: 0.3813962936401367\n",
      "Epoch: 25, Loss: 0.36322641372680664\n",
      "Epoch: 26, Loss: 0.3466953635215759\n",
      "Epoch: 27, Loss: 0.3317846357822418\n",
      "Epoch: 28, Loss: 0.3180895149707794\n",
      "Epoch: 29, Loss: 0.30542829632759094\n",
      "Epoch: 30, Loss: 0.29382333159446716\n",
      "Epoch: 31, Loss: 0.28317365050315857\n",
      "Epoch: 32, Loss: 0.2733510136604309\n",
      "Epoch: 33, Loss: 0.2642778754234314\n",
      "Epoch: 34, Loss: 0.2558552920818329\n",
      "Epoch: 35, Loss: 0.24802826344966888\n",
      "Epoch: 36, Loss: 0.24071554839611053\n",
      "Epoch: 37, Loss: 0.23379220068454742\n",
      "Epoch: 38, Loss: 0.22719712555408478\n",
      "Epoch: 39, Loss: 0.22095628082752228\n",
      "Epoch: 40, Loss: 0.21511724591255188\n",
      "Epoch: 41, Loss: 0.20971360802650452\n",
      "Epoch: 42, Loss: 0.20476093888282776\n",
      "Epoch: 43, Loss: 0.2001686543226242\n",
      "Epoch: 44, Loss: 0.19583173096179962\n",
      "Epoch: 45, Loss: 0.19172057509422302\n",
      "Epoch: 46, Loss: 0.18782228231430054\n",
      "Epoch: 47, Loss: 0.18412531912326813\n",
      "Epoch: 48, Loss: 0.1806527078151703\n",
      "Epoch: 49, Loss: 0.17741350829601288\n",
      "Epoch: 50, Loss: 0.17434780299663544\n",
      "Epoch: 51, Loss: 0.17138193547725677\n",
      "Epoch: 52, Loss: 0.16851350665092468\n",
      "Epoch: 53, Loss: 0.16576127707958221\n",
      "Epoch: 54, Loss: 0.16314801573753357\n",
      "Epoch: 55, Loss: 0.16068154573440552\n",
      "Epoch: 56, Loss: 0.15832969546318054\n",
      "Epoch: 57, Loss: 0.15605434775352478\n",
      "Epoch: 58, Loss: 0.15382122993469238\n",
      "Epoch: 59, Loss: 0.15162721276283264\n",
      "Epoch: 60, Loss: 0.1494840532541275\n",
      "Epoch: 61, Loss: 0.147426575422287\n",
      "Epoch: 62, Loss: 0.1454753279685974\n",
      "Epoch: 63, Loss: 0.143622025847435\n",
      "Epoch: 64, Loss: 0.14183524250984192\n",
      "Epoch: 65, Loss: 0.1400826871395111\n",
      "Epoch: 66, Loss: 0.13835780322551727\n",
      "Epoch: 67, Loss: 0.13666316866874695\n",
      "Epoch: 68, Loss: 0.13502147793769836\n",
      "Epoch: 69, Loss: 0.13344325125217438\n",
      "Epoch: 70, Loss: 0.1319219321012497\n",
      "Epoch: 71, Loss: 0.1304483264684677\n",
      "Epoch: 72, Loss: 0.1290055513381958\n",
      "Epoch: 73, Loss: 0.12758877873420715\n",
      "Epoch: 74, Loss: 0.12620435655117035\n",
      "Epoch: 75, Loss: 0.12486144155263901\n",
      "Epoch: 76, Loss: 0.12356331199407578\n",
      "Epoch: 77, Loss: 0.12231200933456421\n",
      "Epoch: 78, Loss: 0.1211053654551506\n",
      "Epoch: 79, Loss: 0.11993581056594849\n",
      "Epoch: 80, Loss: 0.11879657953977585\n",
      "Epoch: 81, Loss: 0.11768853664398193\n",
      "Epoch: 82, Loss: 0.1166059672832489\n",
      "Epoch: 83, Loss: 0.11554586887359619\n",
      "Epoch: 84, Loss: 0.11450861394405365\n",
      "Epoch: 85, Loss: 0.11349872499704361\n",
      "Epoch: 86, Loss: 0.11251652985811234\n",
      "Epoch: 87, Loss: 0.11156343668699265\n",
      "Epoch: 88, Loss: 0.11063892394304276\n",
      "Epoch: 89, Loss: 0.1097368597984314\n",
      "Epoch: 90, Loss: 0.10885043442249298\n",
      "Epoch: 91, Loss: 0.10797806829214096\n",
      "Epoch: 92, Loss: 0.10712159425020218\n",
      "Epoch: 93, Loss: 0.1062859371304512\n",
      "Epoch: 94, Loss: 0.10547036677598953\n",
      "Epoch: 95, Loss: 0.104670949280262\n",
      "Epoch: 96, Loss: 0.10388030856847763\n",
      "Epoch: 97, Loss: 0.10309801995754242\n",
      "Epoch: 98, Loss: 0.10233097523450851\n",
      "Epoch: 99, Loss: 0.10158449411392212\n",
      "Epoch: 100, Loss: 0.1008591279387474\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9729629629629629"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output = model(im_train)\n",
    "    \n",
    "softmax = torch.exp(output).cpu()\n",
    "prob = list(softmax.numpy())\n",
    "predictions = np.argmax(prob, axis=1)\n",
    "\n",
    "# accuracy on training set\n",
    "accuracy_score(lab_train, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9691666666666666"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output = model(im_val)\n",
    "\n",
    "softmax = torch.exp(output).cpu()\n",
    "prob = list(softmax.numpy())\n",
    "predictions = np.argmax(prob, axis=1)\n",
    "\n",
    "# accuracy on validation set\n",
    "accuracy_score(lab_val, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_test = test_images.reshape(10000, 1, 28, 28).astype(np.float32)\n",
    "im_test = torch.from_numpy(im_test)\n",
    "\n",
    "lab_test = torch.from_numpy(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9725"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output = model(im_test)\n",
    "\n",
    "softmax = torch.exp(output).cpu()\n",
    "prob = list(softmax.numpy())\n",
    "act_predictions = np.argmax(prob, axis=1)\n",
    "\n",
    "accuracy_score(lab_test, act_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ptenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0ae5f5475f8ce4fdbced31bb7e10549994fbb86795b8df593119bef6cd7521a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
